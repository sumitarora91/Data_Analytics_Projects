{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 801a - Intro to Big Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 01 - Web Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>The below code implements a Web Crawler as part of Assignment Exercise 01 for CSE 801a Course.\n",
    "    The logic of this code is to build a program which feeds in a static Web URL as an entry point and from there on implements a SPIDER which crawls through the links present on that website and further the links present on those links (Parent - Child - GrandChild type of implementation)</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*************************************************************************************************</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will follow up with instructions to make a third person help in understanding the logic behind this code</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>First we'll import all the necessary libraries required to implement our Spider (Web Crawler)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse,urljoin\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the below code cell we are supplying the entry point to the crawler (The first link to start the crawler).\n",
    "    Further we are defining 3 empty lists to store the crawled:\n",
    "    <ol>\n",
    "        <li>Source URL</li>\n",
    "        <li>Target URL</li>\n",
    "        <li>Title of Target URL</li>\n",
    "    </ol>\n",
    "    <br>\n",
    "    After that we are extracting the domain of the website and parsing the URL using Netloc to genearte a standard URL to crawl relatively linked external pages in the same domain (home_url will ensure that)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseURL = 'https://en.wikipedia.org/wiki/Manchester_United_F.C.'\n",
    "URL_SOURCE = []\n",
    "URL_TARGET = []\n",
    "URL_TITLE  = []\n",
    "domain = urlparse(BaseURL).netloc\n",
    "parsed_url = urllib.parse.urlparse(BaseURL)\n",
    "home_url = parsed_url.scheme + '://' + parsed_url.netloc #Preparing the Base or Home URL \n",
    "#for the supplied link to access and retrieve the relatively linked webpages within a page\n",
    "parsed_url\n",
    "home_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the below cell we are creating a list to store all the commonly used image extenstion which will further be used to filter the hyperlinks pointing to an image through anchor (< a >) tag rather than an actual webpage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list =['.jpg','.png','jpeg','.svg','.gif','.tif','.tiff','.bmp','.raw','.cr2','.nef','.orf','.sr2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Below cell is used to define a function \"find_title\" which will be used to extract the title of the supplied URL (Title of the target page in our case)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title(title_link):\n",
    "    try:\n",
    "        with urllib.request.urlopen(title_link) as response:\n",
    "            title_links_1 = response.read()\n",
    "            soup_title = BeautifulSoup(title_links_1)\n",
    "            title_of_page = soup_title.title.text\n",
    "    except: #Handling the case in which the call to the URL fails due to any kind of HTTP Error.\n",
    "            #later such URLS with blank title would be skipped/ignored\n",
    "        title_of_page = ''\n",
    "    return title_of_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Bellow cell is implementing a function \"link_list_generator\" which feeds in a URL from where the function is called and further collects all the hyperlinks (External one on same domain) on that URL and returns it back to the point from where the function was called in form of a list of hyperlinks(URLS)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_list_generator(URL_LINK):\n",
    "    with urllib.request.urlopen(URL_LINK) as response:\n",
    "        all_links = response.read()\n",
    "        bs_all_links = BeautifulSoup(all_links)\n",
    "        required_links = set()\n",
    "        for link in bs_all_links.body.find_all('a'):\n",
    "            href = link.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None: #Ignoring URLs without an actual reference\n",
    "                continue\n",
    "            else:\n",
    "                if href.find(\"#\") != -1: #Ignoring internal links that is links to the same page\n",
    "                    continue\n",
    "                else:\n",
    "                    href = urljoin(home_url, href) #Joining the relative URL with home url to obtain actual \n",
    "                                                    #External URL\n",
    "                    if domain in href:         #Checking if the URL is in same domain\n",
    "                        required_links.add(href)\n",
    "    return required_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(URL_SOURCE) #To check if list contains anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below code will append the values for first iteration with the entry point URL</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(URL_SOURCE) == 0): #Updating the list with first iteration as per the specified requirement\n",
    "    URL_SOURCE.append('None')\n",
    "    URL_TARGET.append(BaseURL)\n",
    "    base_url_title = find_title(BaseURL)\n",
    "    #print(f'Base {base_url_title}')\n",
    "    URL_TITLE.append(base_url_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below cell is implementing our main function <i><u>crawler</u></i> which feeds in on a URL then performs certains checks and validations and populates our lists which are of interest to us namely <i>URL_SOURCE</i>, <i>URL_TARGET</i>, and <i>URL_TITLE</i>(for the target page)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(URL):\n",
    "    crawler_count = 0\n",
    "    if (len(URL_SOURCE) > 0):\n",
    "        requested_urls = link_list_generator(URL)\n",
    "        for link1 in requested_urls:\n",
    "            if link1 in URL_SOURCE:\n",
    "                continue #Ignoring links already present in source URL\n",
    "            else:\n",
    "                image_contains = [image_check for image_check in image_list if(image_check in link1)]\n",
    "                image_contains = bool(image_contains) #Checking with the link is of type image instead of actual link\n",
    "                if image_contains == True:\n",
    "                    continue #ignoring imaging type URLS\n",
    "                else:\n",
    "                    if (len(URL_SOURCE) < 100): #Ensuring that 100 URLS are not already crawled\n",
    "                        target_url = link1\n",
    "                        target_title = find_title(target_url)\n",
    "                        if (crawler_count < 20): #Ensuring that only 20 links are crawled per parent URL\n",
    "                                                 #Limited Crawling concept\n",
    "                            if (target_url in URL_SOURCE) or (target_url in URL_TARGET) or (target_title in URL_TITLE):\n",
    "                                continue #Ignoring target URLS already in Source or target and fetching the next ones\n",
    "                            else:\n",
    "                                if target_title == '':\n",
    "                                    continue #Ignoring URLS without a title\n",
    "                                else:\n",
    "                                    #print(f'{crawler_count} : {target_title}')\n",
    "                                    URL_SOURCE.append(URL)\n",
    "                                    URL_TARGET.append(target_url)\n",
    "                                    URL_TITLE.append(target_title)\n",
    "                                    crawler_count += 1\n",
    "                        else:\n",
    "                            if (target_url in URL_SOURCE) or (target_url in URL_TARGET) or (target_title in URL_TITLE):\n",
    "                                continue\n",
    "                            else:\n",
    "                                try:\n",
    "                                    with urllib.request.urlopen(target_url) as response:\n",
    "                                        URL_NEW = target_url\n",
    "                                        crawler(URL_NEW) #Recursively Calling the Crawler function to crawl the remaing\n",
    "                                                            #Urls for every 20 iterations per URL max to ensure\n",
    "                                                            #One parent has only 20 direct childs\n",
    "                                except urllib.error.HTTPError as e:\n",
    "                                    continue\n",
    "                    else:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below cell will implement the logic to call the <i><u>crawler</u></i> function using the entry URL</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: The crawler function might take 1-3 minutes in crawling all the required pages based on the network speeds and how heavy the site being crawled is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler(BaseURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below code transforms each element of list <i><u>URL_SOURCE</u></i> and adds a quotes at the beginning and ending and stores into a new list <i><u>URL_SOURCE_QUOTES</u></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SOURCE_QUOTES = []\n",
    "for list_item1 in URL_SOURCE:\n",
    "    URL_SOURCE_QUOTES.append(f'\"{list_item1}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below code transforms each element of list <i><u>URL_TARGET</u></i> and adds a quotes at the beginning and ending and stores into a new list <i><u>URL_TARGET_QUOTES</u></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TARGET_QUOTES = []\n",
    "for list_item2 in URL_TARGET:\n",
    "    URL_TARGET_QUOTES.append(f'\"{list_item2}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Below code cell converts the list and combines them to form a single pandas dataframe with 3 columns url_source storing the list URL_SOURCE_QUOTES and similarly the others </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web_crawler = pd.DataFrame(list(zip(URL_SOURCE_QUOTES, URL_TARGET_QUOTES, URL_TITLE)),columns=['url_source','url_target', 'page_title_target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> performing print, df.head(), df.tail() to check the list and dataframe contents</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(URL_SOURCE_QUOTES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_source</th>\n",
       "      <th>url_target</th>\n",
       "      <th>page_title_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"None\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>Manchester United F.C. - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Andreas_Pereira\"</td>\n",
       "      <td>Andreas Pereira - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2009_UEFA_Champ...</td>\n",
       "      <td>2009 UEFA Champions League Final - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/1979%E2%80%9380...</td>\n",
       "      <td>1979–80 UEFA Cup - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/North_West_Wome...</td>\n",
       "      <td>North West Women's Regional Football League - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2006%E2%80%9307...</td>\n",
       "      <td>2006–07 UEFA Champions League - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/1991_European_S...</td>\n",
       "      <td>1991 European Super Cup - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Alf_Farman\"</td>\n",
       "      <td>Alf Farman - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/History_of_Manc...</td>\n",
       "      <td>History of Manchester United F.C. (1969–1986) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Manchester_Unit...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Sir_Matt_Busby_...</td>\n",
       "      <td>Sir Matt Busby Player of the Year - Wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          url_source  \\\n",
       "0                                             \"None\"   \n",
       "1  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "2  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "3  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "4  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "5  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "6  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "7  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "8  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "9  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "\n",
       "                                          url_target  \\\n",
       "0  \"https://en.wikipedia.org/wiki/Manchester_Unit...   \n",
       "1    \"https://en.wikipedia.org/wiki/Andreas_Pereira\"   \n",
       "2  \"https://en.wikipedia.org/wiki/2009_UEFA_Champ...   \n",
       "3  \"https://en.wikipedia.org/wiki/1979%E2%80%9380...   \n",
       "4  \"https://en.wikipedia.org/wiki/North_West_Wome...   \n",
       "5  \"https://en.wikipedia.org/wiki/2006%E2%80%9307...   \n",
       "6  \"https://en.wikipedia.org/wiki/1991_European_S...   \n",
       "7         \"https://en.wikipedia.org/wiki/Alf_Farman\"   \n",
       "8  \"https://en.wikipedia.org/wiki/History_of_Manc...   \n",
       "9  \"https://en.wikipedia.org/wiki/Sir_Matt_Busby_...   \n",
       "\n",
       "                                   page_title_target  \n",
       "0                 Manchester United F.C. - Wikipedia  \n",
       "1                        Andreas Pereira - Wikipedia  \n",
       "2       2009 UEFA Champions League Final - Wikipedia  \n",
       "3                       1979–80 UEFA Cup - Wikipedia  \n",
       "4  North West Women's Regional Football League - ...  \n",
       "5          2006–07 UEFA Champions League - Wikipedia  \n",
       "6                1991 European Super Cup - Wikipedia  \n",
       "7                             Alf Farman - Wikipedia  \n",
       "8  History of Manchester United F.C. (1969–1986) ...  \n",
       "9      Sir Matt Busby Player of the Year - Wikipedia  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_web_crawler.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_source</th>\n",
       "      <th>url_target</th>\n",
       "      <th>page_title_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Category:All_st...</td>\n",
       "      <td>Category:All stub articles - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Template:Finlan...</td>\n",
       "      <td>Template:Finland-stub - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Wikipedia:Commu...</td>\n",
       "      <td>Wikipedia:Community portal - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Special:Special...</td>\n",
       "      <td>Special pages - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/w/index.php?title=Sp...</td>\n",
       "      <td>Create account - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Category:Consti...</td>\n",
       "      <td>Category:Constitutions of country subdivisions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/w/index.php?title=Co...</td>\n",
       "      <td>Constitution of Åland - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/w/index.php?title=Sp...</td>\n",
       "      <td>Download as PDF - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Special:RecentC...</td>\n",
       "      <td>Recent changes - Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Constitution_of...</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Special:MyContr...</td>\n",
       "      <td>User contributions for 139.60.123.87 - Wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url_source  \\\n",
       "90  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "91  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "92  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "93  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "94  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "95  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "96  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "97  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "98  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "99  \"https://en.wikipedia.org/wiki/Constitution_of...   \n",
       "\n",
       "                                           url_target  \\\n",
       "90  \"https://en.wikipedia.org/wiki/Category:All_st...   \n",
       "91  \"https://en.wikipedia.org/wiki/Template:Finlan...   \n",
       "92  \"https://en.wikipedia.org/wiki/Wikipedia:Commu...   \n",
       "93  \"https://en.wikipedia.org/wiki/Special:Special...   \n",
       "94  \"https://en.wikipedia.org/w/index.php?title=Sp...   \n",
       "95  \"https://en.wikipedia.org/wiki/Category:Consti...   \n",
       "96  \"https://en.wikipedia.org/w/index.php?title=Co...   \n",
       "97  \"https://en.wikipedia.org/w/index.php?title=Sp...   \n",
       "98  \"https://en.wikipedia.org/wiki/Special:RecentC...   \n",
       "99  \"https://en.wikipedia.org/wiki/Special:MyContr...   \n",
       "\n",
       "                                    page_title_target  \n",
       "90             Category:All stub articles - Wikipedia  \n",
       "91                  Template:Finland-stub - Wikipedia  \n",
       "92             Wikipedia:Community portal - Wikipedia  \n",
       "93                          Special pages - Wikipedia  \n",
       "94                         Create account - Wikipedia  \n",
       "95  Category:Constitutions of country subdivisions...  \n",
       "96                  Constitution of Åland - Wikipedia  \n",
       "97                        Download as PDF - Wikipedia  \n",
       "98                         Recent changes - Wikipedia  \n",
       "99   User contributions for 139.60.123.87 - Wikipedia  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_web_crawler.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This is the final step of the Spider/Crawler which converts the dataframe and stores it in a CSV file <i>WebCrawler.csv</i> with encoding as UTF-8 for the viewing purpose of the user</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web_crawler.to_csv('crawl.csv', index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> There is a bug that I have identified with python. Even if we do to_csv on a dataframe in Jupyter Notebooks and supply the encoding as <i>UTF-8</i> the windows machine overwrites it with its own equivalent charset of cp1252 so we have to live with it. But cp1252 is almost similar as the UTF-8 charset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
